# -*- coding: utf-8 -*-
"""ML Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bMRA2xiAagAMKhp00a1waNTrvCDWXByk

# Machine Learning 2 Project


---



*   Samar Kintab     443003122
*   Lamar Felemban   444003576
*   Jana Shata       444003518
*   Rana Alotmi      443011930

#### Imports
"""

!pip install category_encoders
from IPython.display import clear_output

# Install Kaggle API
!pip install kaggle

# Configure Kaggle API credentials directly in the script
import os

clear_output()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
import numpy as np
import category_encoders as ce
import re
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, GridSearchCV, KFold
from sklearn.metrics import make_scorer, recall_score,precision_score, f1_score, accuracy_score, confusion_matrix, classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from IPython.display import display
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_selection import mutual_info_classif, RFE

"""# **1. Data Preprocessing**

### Uploading the data
"""

os.environ['KAGGLE_USERNAME'] = "mokaron"
os.environ['KAGGLE_KEY'] = "d57cbb06de4d89f8525770468c2cd65c"

# Replace with the dataset URL path from Kaggle
dataset_name = "hopesb/student-depression-dataset"

# Download and unzip the dataset
!kaggle datasets download -d {dataset_name} --unzip --force

# List files in the dataset directory
import os
print("Downloaded files:", os.listdir())

data=pd.read_csv('Student Depression Dataset.csv')

"""### Basic data exploration"""

print(f"Data shape: {data.shape[0]} rows, {data.shape[1]} columns")
data.head()

#  Basic Info and Data Types
print("Basic Dataset Information:")
data.info()

"""### Handling missing values"""

#  Missing Value Analysis
print("\nMissing Value Counts:")
print(data.isnull().sum())

"""It appears that only a single column "*Financial Stress*" contains missing values that needs to be handled. As the number of missing values it pretty small, we choose to imoute them with the *mean* value, as it avoids the need of dropping whole rows becuase of a single missing feature, and will not have any significant impact on the data distribution."""

# Filling Na (missing) values with the mean of the column
data['Financial Stress'] = data['Financial Stress'].fillna(value= data['Financial Stress'].mean(skipna=True))

# Checking the column status
print(data['Financial Stress'].isnull().sum())

"""## Handling duplicate rows"""

data.duplicated().sum()

"""The above cell denotes that the dataset does not contain any duplicated rows

### Dropping redundant columns
"""

# Count unique values in each column
unique_counts = data.nunique()
print("\nUnique Values Count:\n", unique_counts)

""""*id*" and "*City*" columns are categorical features with many values. Our insight is to drop them because the information they hold is irrelevant to the problem at hand, whuch is predicting student depression."""

data.drop(['id', 'City'], axis=1, inplace=True)

print('current data shape:', data.shape)
data.head()

data[['Work Pressure', 'Job Satisfaction']].value_counts()

"""These two features have majority of their values set to 0, except for very few observations. We find such phenomena irrelevent to the contribution of the classification problem. In addition, it might even lead to problems such as overfitting, so we decide to drop them as well."""

data.drop(['Work Pressure', 'Job Satisfaction'], axis=1, inplace=True)

print('current data shape:', data.shape)
data.head()

"""### Handling outliers"""

# Step 1: Identify numerical columns (excluding 'Depression')
numerical_columns = data.select_dtypes(include=['number']).columns
numerical_columns = [col for col in numerical_columns if col.lower() != 'depression']  # Exclude 'Depression'

# Step 2: Outlier Detection using IQR method
def detect_outliers_iqr(df, numerical_columns):
    outliers = {}
    for col in numerical_columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outlier_data = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        if len(outlier_data) != 0:
          outliers[col] = outlier_data
    return outliers

# Get outliers based on IQR method
numerical_outliers = detect_outliers_iqr(data, numerical_columns)

# Print outliers for numerical columns
print(f"Outliers detected in columns:")
for col, outlier_data in numerical_outliers.items():
    print(f"{col}: {len(outlier_data)} outleris")

# Step 3: Create a Boxplot for all numerical columns (excluding 'id')
plt.figure(figsize=(12, 6))
sns.boxplot(data=data[numerical_columns])  # Only numerical columns excluding 'id'
plt.title('Outliers in Numerial Columns', fontsize=16)
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability
plt.tight_layout()  # Adjust layout for better spacing

plt.savefig("outliers.jpg", dpi=300, bbox_inches="tight")
plt.show()

"""Based on the analysis of the problem's specification and outliers' visualization, we derived the following insights:
- The number of outliers is very insignificant, only 12 and 9 out of 27901 total observations.
- The range of the values isn't very irregular, it can be interpreted as the data following a slightly skewed normal distribution.

For the previous reasons, we decided to leave the current data as is, because manipulating them could defect more than affect the classification problem.

### Encoding categorical features
"""

# Identify object columns (excluding 'Sleep Duration', because it needs more specific processing)
object_cols = data.select_dtypes(include=['object']).columns
object_cols = [col for col in object_cols if col != 'Sleep Duration']  # Exclude 'Sleep Duration'

# Examining the number of unique values per column, as it affects the encoding method
data[object_cols].nunique()

data[object_cols].head()

"""Methodology:
- **Yes/No columns** (*Family history of mental illness* & *Suicidal thoughts*): **Convert to 1/0**
- **Nominal columns *with low cardinality*** (*Gender*): **One hot encoding** `For more interpretability`
- **Nominal columns *with high cardinality*** (*Profession* & *Degree*): **Binary encoding** `To avoid creating sparse columns with high dimensionality`
- **Ordinal columns** (*Dietary Habits*): **Label encoding** `To preserve meaningful orderings`
"""

# Gender
data['Gender_Female'] = (data['Gender'] == 'Female').astype(int)
data['Gender_Male'] = (data['Gender'] == 'Male').astype(int)
data.drop('Gender', axis=1, inplace=True)

# Suicidal thoughts
data['Have you ever had suicidal thoughts ?'] = (data['Have you ever had suicidal thoughts ?'] == 'Yes').astype(int)

# Family History of Mental Illness
data['Family History of Mental Illness'] = (data['Family History of Mental Illness'] == 'Yes').astype(int)

# Dietary Habits
diet_habits = {'Others': -1, 'Unhealthy': 0, 'Moderate': 1, 'Healthy': 2}
data['Dietary Habits'] = data['Dietary Habits'].map(diet_habits)

# Profession & Degree
binary_encoder = ce.BinaryEncoder(cols=['Profession', 'Degree'])
data = binary_encoder.fit_transform(data)

print('current data shape:', data.shape)
data.head()

"""For the *Sleep Duration* column, whose values are alphanumerical, we use regular expressions (ReGex) to extract the numercal parts and asssign their mean to be the entry value."""

def mean_mapper(st):
  st_nums_list = re.findall('[0-9]', st)
  int_nums_list = list(map(int, st_nums_list))
  mean = np.mean(int_nums_list);
  if not np.isnan(mean):
    return mean
  else:
    return 0

data['Sleep Duration'] = data['Sleep Duration'].map(mean_mapper)

print(data.info())
data.head()

print(data['Depression'].value_counts())

"""Based on this we observed that **the data is unbalanced**

## Oversample Data Using SMOTE
"""

# Separate features and target
X = data.drop('Depression', axis=1)
y = data['Depression']

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X, y)

# Combine balanced features and target into a new DataFrame
Oversampled_data = pd.concat([pd.DataFrame(X_smote, columns=X.columns), pd.Series(y_smote, name='Depression')], axis=1)

# Check the new class distribution
print(Oversampled_data['Depression'].value_counts())

"""# **2. Data Visualization & Analysis**

### **Gender Distribution**
"""

# Extracting the values for the pie chart
values = [Oversampled_data['Gender_Female'].sum(), Oversampled_data['Gender_Male'].sum()]
labels = ['Female', 'Male']
colors = ['pink', 'blue']

# Creating the pie chart
plt.figure(figsize=(8, 8))
plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors)
plt.title('Gender Distribution')
plt.savefig("Gender Distribution.jpg", dpi=300, bbox_inches="tight")
plt.show()

"""This pie chart illustrates the **gender distribution** within the dataset. The chart reveals that the dataset consists of 55.7% males and 44.3% females. This indicates a **slight predominance of male participants** compared to female participants.

### **Depression Based on Gender**
"""

# Grouping the data by Gender and Depression and getting counts
gender_depression_counts = Oversampled_data.groupby(['Gender_Male', 'Depression']).size().reset_index(name='Count')

# Extracting the values for the pie chart
values = [
    gender_depression_counts[(gender_depression_counts['Gender_Male'] == 0) & (gender_depression_counts['Depression'] == 1)]['Count'].values[0],  # Females with depression
    gender_depression_counts[(gender_depression_counts['Gender_Male'] == 1) & (gender_depression_counts['Depression'] == 1)]['Count'].values[0],  # Males with depression
    gender_depression_counts[(gender_depression_counts['Gender_Male'] == 1) & (gender_depression_counts['Depression'] == 0)]['Count'].values[0],   # Males without depression
    gender_depression_counts[(gender_depression_counts['Gender_Male'] == 0) & (gender_depression_counts['Depression'] == 0)]['Count'].values[0]  # Females without depression
]

labels = ['Females with Depression', 'Males with Depression', 'Males without Depression', 'Females without Depression']
colors = ['lightcoral', 'blue', 'lightblue', 'pink']

# Creating the pie chart
plt.figure(figsize=(8, 8))
plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors)
plt.title('Depression Distribution by Gender')
plt.savefig("Depression Distribution by Gender.jpg", dpi=300, bbox_inches="tight")
plt.show()

"""This pie chart shows the **depression distribution by gender**, dividing individuals into males and females with and without depression. **Males with depression make up the largest group** (32.7%), followed by females with depression (25.9%). **Females without depression are the smallest group**, hinting at higher depression rates among females.

*   Given that females represent only 44.3% of the dataset, their depression ratios are proportionally higher when adjusted for group size.
"""

print(len(numerical_columns))

excluded_features = ['Depression', 'Gender_Male', 'Degree_0', 'Degree_1', 'Degree_2', 'Degree_3', 'Degree_4', 'Profession_0', 'Profession_1', 'Profession_2', 'Profession_3']
numerical_columns = [feat for feat in Oversampled_data.columns if feat not in excluded_features]

class_col = 'Depression'

# Create subplots (4 rows, 3 columns)
fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(16, 12))
axes = axes.flatten()  # Convert 2D array into 1D for easy indexing

# Plot KDEs
for i, feature in enumerate(numerical_columns):
    sns.kdeplot(data=Oversampled_data, x=feature, hue=class_col, fill=True, common_norm=False, alpha=0.3, ax=axes[i])
    axes[i].set_title(f'Distribution of {feature} by {class_col}')

# Hide unused subplots (if features < 12)
for j in range(len(numerical_columns), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""### **Numerical Feature Distributions**"""

warnings.filterwarnings("ignore")

# Select a few important features
excluded_features = ['Gender_Male', 'Degree_0', 'Degree_1', 'Degree_2', 'Degree_3', 'Degree_4', 'Profession_0', 'Profession_1', 'Profession_2', 'Profession_3']
numerical_columns = [feat for feat in Oversampled_data.columns if feat not in excluded_features]

sns.set(style="darkgrid")

# Set up the figure size for numerical column distribution plots
num_cols = len(numerical_columns)
num_rows = (num_cols // 3) + (num_cols % 3 > 0)

# Set up the figure size and grid for subplots
fig, axes = plt.subplots(nrows=num_rows, ncols=3, figsize=(15, 5 * num_rows))
axes = axes.flatten()

colors = sns.color_palette("Purples", num_cols)

dark_colors = sns.color_palette("Purples", 10)[7:10]  # Dark purples for the first three columns
colors[:3] = dark_colors  # Assign dark colors to the first three

# Plot numerical columns (Distribution)
for i, col in enumerate(numerical_columns):
    sns.histplot(Oversampled_data[col], kde=True, bins=20, color=colors[i], ax=axes[i], alpha=0.7)
    axes[i].set_title(f'{col} Distribution', fontsize=14, fontweight='bold', color='black')
    axes[i].set_xlabel(col, fontsize=12, color='black')
    axes[i].set_ylabel('Frequency', fontsize=12, color='black')
    axes[i].tick_params(axis='both', which='major', labelsize=10, colors='black')
    axes[i].set_facecolor('#e0e0e0')  # Light gray background for each subplot

# Remove any empty subplots if the number of columns isn't a multiple of 3
for i in range(len(numerical_columns), len(axes)):
    fig.delaxes(axes[i])

# show the plot
plt.tight_layout()
plt.savefig("Numerical Distributions.jpg", dpi=300, bbox_inches="tight")
plt.show()

"""## **KDE Plot**"""

excluded_features = ['Depression', 'Gender_Male', 'Degree_0', 'Degree_1', 'Degree_2', 'Degree_3', 'Degree_4', 'Profession_0', 'Profession_1', 'Profession_2', 'Profession_3']
numerical_columns = [feat for feat in Oversampled_data.columns if feat not in excluded_features]

class_col = 'Depression'

# Create subplots (4 rows, 3 columns)
fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(16, 12))
axes = axes.flatten()  # Convert 2D array into 1D for easy indexing

# Plot KDEs
for i, feature in enumerate(numerical_columns):
    sns.kdeplot(data=Oversampled_data, x=feature, hue=class_col, fill=True, common_norm=False, alpha=0.3, ax=axes[i],
                palette={0: 'skyblue', 1: 'red'})
    axes[i].set_title(f'Distribution of {feature} by {class_col}')

# Hide unused subplots (if features < 12)
for j in range(len(numerical_columns), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.savefig("Depression Distribution by Numerical Features.jpg", dpi=300, bbox_inches="tight")
plt.show()

"""# **3. Feature Selection**"""

X = Oversampled_data.drop('Depression', axis=1)  # Exclude the target column 'Depression'
y = Oversampled_data['Depression']  # Target column

print(f"Data shape: {data.shape[0]} rows, {data.shape[1]} columns")
data.head()

# Split the dataset into 75% for training and 25% for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

"""#### Training the model on all fetuers for Performance Evaluation."""

# Train the model using all features
model_all = RandomForestClassifier(random_state=42)
model_all.fit(X_train, y_train)

# Predict the results on the test set using all features
y_pred_all = model_all.predict(X_test)

# Calculate the model performance using all features
accuracy_all = accuracy_score(y_test, y_pred_all)
precision_all = precision_score(y_test, y_pred_all)
recall_all = recall_score(y_test, y_pred_all)

# Display the model performance using all features
print("\nPerformance using All Features:")
print(classification_report(y_test, y_pred_all))

"""#### Identifying the most relevant features using these techniques: **correlation analysis**, **mutual information**, **Recursive Feature Elimination (RFE)**.

#### a) correlation analysis.
"""

correlation_matrix = Oversampled_data.corr()

plt.figure(figsize=(14, 10))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar=True)
plt.title('Correlation Matrix')
plt.show()

"""The correlation matrix heatmap illustrates the correlation coefficient of all feature with eachother, along with the target "***Depression***". A number of interesting insights are visible.

For instance, it appears that new crafted colomns (like Profession_0 to Profession_3, and Degree_0 to Degree_4) have a certain noticeable correlation with each other, delivering a good impact about the quality of choices in the feature-engineering process.

Moreover, it's evident that only a limited number of features correlate with the target, emphasizing the need for *feature selection*.

From the correlation matrix heatmap, We noticed that 7 features are strongly correlated with the target column, which is depression. Based on this observation, using techniques like Mutual Information (MI) and Recursive Feature Elimination (RFE), We selected these top 7 features since they had the most significant impact on predicting depression.

#### b) mutual information (MI).
"""

# Selecting the best features using Mutual Information
mi_scores = mutual_info_classif(X_train, y_train)

# Sorting the features in ascending order
mi_scores_series = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)

# Plotting a bar chart to display MI scores for each feature
plt.figure(figsize=(10, 6))
sns.barplot(x=mi_scores_series.values, y=mi_scores_series.index)
plt.xlabel("Mutual Information Score")
plt.ylabel("Features")
plt.title("Feature Ranking using Mutual Information")
plt.show()

# Defining the model
model = RandomForestClassifier(random_state=42)

# Selecting the top 9 features based on MI
selected_features_mi = mi_scores_series.index[:9]
print("Top 9 features using MI:")
print(selected_features_mi.tolist())

# Training the model using the selected features from MI
model_mi = RandomForestClassifier(random_state=42)
model_mi.fit(X_train[selected_features_mi], y_train)

# Predicting the results on the test set
y_pred_mi = model_mi.predict(X_test[selected_features_mi])

# Calculating performance for the model
accuracy_mi = accuracy_score(y_test, y_pred_mi)
precision_mi = precision_score(y_test, y_pred_mi)
recall_mi = recall_score(y_test, y_pred_mi)

# Displaying the performance of the model using the selected features
print("\nClassification Report for MI-selected Features:")
print(classification_report(y_test, y_pred_mi))

"""#### c) Recursive Feature Elimination (RFE)."""

# Selecting the number of features to be 9 using RFE
rfe = RFE(model, n_features_to_select=9)

# Training the model using RFE
rfe.fit(X_train, y_train)

# Selecting only the features with rank 1
selected_features_rfe = X.columns[rfe.support_]

# Plotting a bar chart to display the RFE ranking for each feature
rfe_ranking = pd.Series(rfe.ranking_, index=X.columns).sort_values(ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(x=rfe_ranking.values, y=rfe_ranking.index)
plt.xlabel("RFE Ranking")
plt.ylabel("Features")
plt.title("Feature Ranking using RFE")
plt.show()

# Training the model using the selected features from RFE
model_rfe = RandomForestClassifier(random_state=42)
model_rfe.fit(X_train[selected_features_rfe], y_train)

print("\nTop features using RFE (Rank = 1):")
print(selected_features_rfe.tolist())

# Predicting the results on the test set
y_pred_rfe = model_rfe.predict(X_test[selected_features_rfe])

# Calculating performance for the model
accuracy_rfe = accuracy_score(y_test, y_pred_rfe)
precision_rfe = precision_score(y_test, y_pred_rfe)
recall_rfe = recall_score(y_test, y_pred_rfe)

print("\nClassification Report for RFE-selected Features:")
print(classification_report(y_test, y_pred_rfe))

"""### Drop ineffective features and keep features selected by RFE."""

# List of selected features to keep
selected_features = ['Age', 'Academic Pressure', 'CGPA', 'Study Satisfaction',
                     'Sleep Duration', 'Dietary Habits', 'Have you ever had suicidal thoughts ?',
                     'Work/Study Hours', 'Financial Stress']

# Dropping unnecessary features
X_selected = X_smote[selected_features]

# Merging selected features with the target variable (Depression)
Oversampled_selected_data = pd.concat([X_selected, pd.Series(y_smote, name='Depression')], axis=1)

# Print the number of rows and columns
num_rows, num_columns = Oversampled_selected_data.shape
print(f"Number of Rows: {num_rows}, Number of Columns: {num_columns}")

# Display the first 5 rows of the modified dataset
display(Oversampled_selected_data.head())

"""#### **summary of Feature Selection Analysis and Its Impact on Model Performance**

The features were selected based on a correlation matrix analysis to determine their relationship with the target (depression). The results showed that some features were highly correlated, while others had weaker correlations but remained important for the model. Initially, 7 features were selected based on their strong correlation. However, experiments revealed that using 9 features provided a better balance between performance and efficiency. Increasing the number to 10 or 11 features did not result in additional improvement, confirming that 9 features were the optimal number.

<br>

**Performance Analysis and Feature Comparison**

The feature selection process using MI and RFE produced identical results, as both methods selected the same features. This highlights the importance of these features and their strong relationship with the target (depression), reinforcing the confidence in their effectiveness for improving model performance.

When comparing all original features (All Features) with those selected using MI and RFE, the overall performance was similar. The Accuracy for All Features and RFE was 0.86, while it was slightly lower for MI (0.85).

For the depressed class (1), the Recall was identical across All Features, RFE, and MI (0.86). Regarding Precision, All Features and RFE were equal (0.86), while MI was slightly lower (0.85).

For the non-depressed class (0), All Features slightly outperformed RFE in Recall (0.86 vs. 0.85), while the other metrics, such as Precision and F1-score, were identical between them. Despite the slight advantage of All Features in Recall for the non-depressed class, the difference was minimal and practically insignificant, especially given the priority in medical datasets to identify depressed cases (class 1).

**RFE delivered performance equivalent to All Features while reducing the number of features, making it more efficient without compromising result quality. On the other hand, MI was less efficient in Precision and Accuracy, making it less suitable for this dataset.**

# **4. Algorithms Comparision**

## Splitting the dataset
"""

X = Oversampled_selected_data.drop('Depression', axis=1)
y = Oversampled_selected_data['Depression']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16, shuffle=True)

"""### 1) SVM"""

# Set up possible values of parameters to optimize over
#p_grid = {"C": [1, 10, 100], "gamma": [0.01, 0.1]}

# We will use a Support Vector Classifier with "rbf" kernel
#svc_model = SVC(kernel='rbf')

# Performing Grid-Search to find the optimal parameters
#clf_svc = GridSearchCV(estimator=svc_model, param_grid=p_grid, cv=5)
#clf_svc.fit(X_train, y_train)

# Printing the optimal parameters
#print(clf_svc.best_params_)

# Reconstructing the model with the optimal parameters to cut the search time in next runs
svm_model = SVC(kernel='rbf', C=1, gamma=0.01).fit(X_train, y_train)

# Making predictions
y_pred = svm_model.predict(X_test)

# Evaluating performance
print(classification_report(y_test, y_pred))

"""### 2) K-Nearest Neighbors (KNN)"""

# Find the best k value using cross-validation
k_range = range(1, 21)
cv_scores = []

# Loop through different k values and perform cross-validation
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')  # Using accuracy as the scoring metric
    cv_scores.append(scores.mean())

# Get the best k value (highest cross-validation accuracy)
best_k = k_range[np.argmax(cv_scores)]
print(f"The best k is: {best_k}")

# Plot the cross-validation results to see the impact of k on accuracy
plt.figure(figsize=(8, 6))
plt.plot(k_range, cv_scores, marker='o', color='b', linestyle='-', markersize=8)
plt.title('KNN Hyperparameter Tuning: k vs. Accuracy (Cross-Validation)')
plt.xlabel('k (Number of Neighbors)')
plt.ylabel('Cross-Validation Accuracy')
plt.grid(True)
plt.xticks(np.arange(1, 21, step=1))  # Display k values on x-axis
plt.show()

"""The plot shows that as k increases in the KNN algorithm, accuracy improves until it stabilizes around k=7. Beyond this point, there’s little to no gain in accuracy, indicating that k=7 is the optimal value for this model."""

# Train the KNN model with the best k on the training data
knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(X_train, y_train)

# Predict the results on the test set
y_pred_rfe = knn.predict(X_test)

# Print the accuracy and classification report
print("\nClassification Report for KNN Model:")
print(classification_report(y_test, y_pred_rfe))

"""### 3) Decision Tree"""

# Initialize the Decision Tree model (Gini index used by default)
dt_model = DecisionTreeClassifier(criterion='gini', random_state=42)

# Train the model using the training set
dt_model.fit(X_train, y_train)

# Predict using the test set
y_pred_dt = dt_model.predict(X_test)

# Calculate performance metrics
accuracy_dt = accuracy_score(y_test, y_pred_dt)
precision_dt = precision_score(y_test, y_pred_dt)
recall_dt = recall_score(y_test, y_pred_dt)

# Display performance results
print("\nDecision Tree Classifier Performance:")
print(classification_report(y_test, y_pred_dt))

"""### 4) Random Forest"""

# Initialize the Random Forest model
rf_model = RandomForestClassifier(random_state=16, n_estimators=100)

# Train the model using the training set
rf_model.fit(X_train, y_train)

# Predict using the test set
y_pred_rf = rf_model.predict(X_test)

# Calculate performance metrics
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)

# Display performance results
print("\nRandom Forest Classifier Performance:")
print(classification_report(y_test, y_pred_rf))

"""## **Visualization to the performance of all models**"""

# Initialize models
models = {
    'SVC': SVC(kernel='rbf', C=1, gamma=0.01),
    'KNN': KNeighborsClassifier(n_neighbors=5),  # Using the best k from previous search
    'Decision Tree': DecisionTreeClassifier(criterion='gini', random_state=42),
    'Random Forest': RandomForestClassifier(random_state=16, n_estimators=100)
}

# Store metrics for each model
metrics = {
    'Model': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1 Score': []
}

# Train models and compute metrics
for model_name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)

    # Predict on the test set
    y_pred = model.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    # Calculate average recall (macro average)
    recall = recall_score(y_test, y_pred, average='macro')
    f1 = f1_score(y_test, y_pred)

    # Print the Recall for each model
    print(f"Recall for {model_name}: {recall:.4f}")

    # Append results to the metrics dictionary
    metrics['Model'].append(model_name)
    metrics['Accuracy'].append(accuracy)
    metrics['Precision'].append(precision)
    metrics['Recall'].append(recall)
    metrics['F1 Score'].append(f1)

# Convert metrics to numpy arrays for easy manipulation
metrics_np = {key: np.array(value) for key, value in metrics.items()}

# Plot the results
fig, ax = plt.subplots(figsize=(10, 6))

# Define the index for each model
index = np.arange(len(models))

# Plot each metric with custom colors
bar_width = 0.2
opacity = 0.8
colors = ['#FADCD9', '#F6F3A7', '#D9A7F7', '#A7D9F7']  # Baby Pink, Light Yellow, Light Purple, Baby Blue

rects1 = plt.bar(index, metrics_np['Accuracy'], bar_width, alpha=opacity, label='Accuracy', color=colors[0])
rects2 = plt.bar(index + bar_width, metrics_np['Precision'], bar_width, alpha=opacity, label='Precision', color=colors[1])
rects3 = plt.bar(index + 2 * bar_width, metrics_np['Recall'], bar_width, alpha=opacity, label='Recall', color=colors[2])
rects4 = plt.bar(index + 3 * bar_width, metrics_np['F1 Score'], bar_width, alpha=opacity, label='F1 Score', color=colors[3])

# Customize the plot
plt.xlabel('Model')
plt.ylabel('Scores')
plt.title('Model Performance Comparison')
plt.xticks(index + 1.5 * bar_width, metrics_np['Model'], rotation=45)

# Move the legend outside the plot to avoid overlap
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))

# Set the y-axis limit to start from 0.6
plt.ylim(0.6, 1.0)

# Display the plot
plt.tight_layout()
plt.show()